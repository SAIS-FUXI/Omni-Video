# Model Architecture Parameters
model:
  t5:
    checkpoint: 'models_t5_umt5-xxl-enc-bf16.pth'
    tokenizer: 'google/umt5-xxl'
  vae:
    checkpoint: 'Wan2.1_VAE.pth'
    stride: [4, 8, 8]
  transformer:
    patch_size: [1, 2, 2]
    dim: 1536
    ffn_dim: 8960
    freq_dim: 256
    num_heads: 12
    num_layers: 30
    window_size: [-1, -1]
    qk_norm: true
    cross_attn_norm: true
    eps: 1e-6
  ar_vision_head:
    learnable_query_length: 4
  adapter:
    in_channels: 1152
    out_channels: 4096
    query_length: 256
  visual_context_adapter:
    visual_context_adapter_patch_size: [1, 4, 4]
  
  num_train_timesteps: 1000
  param_dtype: "float32"

# Training Configuration
training:
  # Model and Adapter Parameters
  model_settings:
    dit_fsdp: false
    use_usp: false
    train_wan_model: true
    train_adapter: false
    train_visual_context_adapter: true
    train_ar_vision_head: false
    replace_context_with_adapter: false
    use_visual_as_input: false
    condition_mode: "full"
    max_context_len: 2560
    max_prompt_len: 256
    max_ref_len: 256
    max_ar_vision_len: 1024
    max_visual_emb_len: 1024
    compute_region_weights: false
    flow_shift: 3.0

    weighting_scheme: "logit_normal"
    logit_mean: 0.0
    logit_std: 1.0
    mode_scale: 1.29

  # Training Hyperparameters
  hyperparameters:
    batch_size: 16
    gradient_accumulation_steps: 1
    num_epochs: 100
    learning_rate: 3e-6
    weight_decay: 0.0001
    max_grad_norm: 0.1
    num_warmup_steps: 500

  # Logging and Checkpointing
  logging:
    log_interval: 50
    save_interval: 1000
    use_tensorboard: true

  # Precision and Optimization
  precision:
    mixed_precision: "bf16"  # Options: "fp32", "fp16", "bf16"
  optimization:
    gradient_checkpointing: true

  # DeepSpeed Settings
  deepspeed:
    enabled: true
    zero_stage: 1
    offload_optimizer: false
    offload_param: false
    config_path: "configs/train_config/zero1_config.json"

  # Classifier-free Guidance
  classifier_free:
    ratio: 0.2
    unconditioned_context_length: 256
    unconditioned_context_path: "unconditioned_context/context.pkl"
  special_tokens:
    path: "special_tokens/tokens.pkl"
    enabled: true

  # Dataloaders Configuration
  dataloaders:
    t2i:
      data_file: "examples/finetune_data/t2i_sample_paths.txt"
      batch_size: 2
      weight: 1
      num_workers: 2
      shuffle: true
      drop_last: true
    i2i:
      data_file: "examples/finetune_data/i2i_sample_paths.txt"
      batch_size: 2
      weight: 1
      num_workers: 2
      shuffle: true
      drop_last: true
    t2v:
      data_file: "examples/finetune_data/t2v_sample_paths.txt"
      batch_size: 2
      weight: 1
      num_workers: 2
      shuffle: true
      drop_last: true
